{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99968201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f173b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"yelp_review.csv\"\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11381b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'stars']].dropna()\n",
    "df.rename(columns={'text': 'review', 'stars': 'actual_stars'}, inplace=True)\n",
    "\n",
    "print(\"After cleaning:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e23819",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 200\n",
    "\n",
    "df_sample = df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_sample['actual_stars'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e13a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"\\n--- Review {i+1} ---\")\n",
    "    print(df_sample.loc[i, 'review'][:300])\n",
    "    print(\"Actual stars:\", df_sample.loc[i, 'actual_stars'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b505937",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_rows = []\n",
    "print(\"Evaluation structure ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56421f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenRouter LLM call setup (OpenAI‚Äëcompatible)\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Ensure you have set your OpenRouter API key in environment\n",
    "# e.g., export OPENROUTER_API_KEY=\"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\"\n",
    "openrouter_key = \"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\"\n",
    "\n",
    "# Initialize OpenAI client to point to OpenRouter endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\", \n",
    "    api_key=openrouter_key\n",
    ")\n",
    "\n",
    "# Choose a model available via OpenRouter\n",
    "MODEL_NAME = \"openrouter/auto\"  # let OpenRouter automatically pick a free model\n",
    "# Alternative explicit free models if you want:\n",
    "# MODEL_NAME = \"mistralai/mistral-7b-instruct:free\"\n",
    "# MODEL_NAME = \"anthropic/claude-3-haiku:free\"\n",
    "\n",
    "def call_llm_openrouter(prompt_text):\n",
    "    \"\"\"\n",
    "    Sends prompt to OpenRouter and returns raw text response.\n",
    "    Stops execution on credit errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert sentiment analyst.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=150  # üî• MUST be low for free/low credits\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "\n",
    "        # üö® Stop execution if credits are exhausted\n",
    "        if \"402\" in error_msg or \"Insufficient credits\" in error_msg:\n",
    "            print(\"‚ùå OPENROUTER CREDITS EXHAUSTED ‚Äî STOPPING EXECUTION\")\n",
    "            raise SystemExit()\n",
    "\n",
    "        print(\"LLM call failed:\", error_msg)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_safe(llm_output):\n",
    "    \"\"\"\n",
    "    Safely parse JSON from LLM output.\n",
    "    Returns (parsed_dict, valid_flag)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fix common JSON issues (like single quotes ‚Üí double quotes)\n",
    "        clean_output = llm_output.replace(\"'\", '\"')\n",
    "        parsed = json.loads(clean_output)\n",
    "        \n",
    "        # Ensure predicted_stars exists and is 1-5\n",
    "        stars = parsed.get(\"predicted_stars\", None)\n",
    "        if stars is None or not (1 <= int(stars) <= 5):\n",
    "            return None, False\n",
    "        \n",
    "        return parsed, True\n",
    "    except Exception as e:\n",
    "        return None, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Prompt V1 text\n",
    "with open(\"prompts/prompt_v1.txt\", \"r\") as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "evaluation_rows = []\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    review_text = row[\"review\"]\n",
    "    actual_stars = row[\"actual_stars\"]\n",
    "\n",
    "    prompt_filled = prompt_template.replace(\"{review_text}\", review_text)\n",
    "\n",
    "    # Call OpenRouter LLM\n",
    "    llm_output = call_llm_openrouter(prompt_filled)\n",
    "\n",
    "    # ‚õî Stop entire loop if credits are exhausted\n",
    "    if llm_output is None:\n",
    "        print(\"Stopping loop due to API failure / no credits.\")\n",
    "        break\n",
    "\n",
    "    # Parse JSON safely\n",
    "    parsed_json, valid_flag = parse_json_safe(llm_output)\n",
    "\n",
    "    evaluation_rows.append({\n",
    "        \"prompt_version\": \"v1\",\n",
    "        \"review\": review_text,\n",
    "        \"actual_stars\": actual_stars,\n",
    "        \"predicted_stars\": parsed_json.get(\"predicted_stars\") if parsed_json else None,\n",
    "        \"explanation\": parsed_json.get(\"explanation\") if parsed_json else None,\n",
    "        \"json_valid\": valid_flag,\n",
    "        \"llm_raw_output\": llm_output\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18f67c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_version</th>\n",
       "      <th>review</th>\n",
       "      <th>actual_stars</th>\n",
       "      <th>predicted_stars</th>\n",
       "      <th>explanation</th>\n",
       "      <th>json_valid</th>\n",
       "      <th>llm_raw_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v1</td>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v1</td>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v1</td>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v1</td>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v1</td>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_version                                             review  \\\n",
       "0             v1  We got here around midnight last Friday... the...   \n",
       "1             v1  Brought a friend from Louisiana here.  She say...   \n",
       "2             v1  Every friday, my dad and I eat here. We order ...   \n",
       "3             v1  My husband and I were really, really disappoin...   \n",
       "4             v1  Love this place!  Was in phoenix 3 weeks for w...   \n",
       "\n",
       "   actual_stars predicted_stars explanation  json_valid llm_raw_output  \n",
       "0             4            None        None       False                 \n",
       "1             5            None        None       False                 \n",
       "2             3            None        None       False                 \n",
       "3             1            None        None       False                 \n",
       "4             5            None        None       False                 "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(evaluation_rows)\n",
    "results_df.to_csv(\"results/evaluation_v1.csv\", index=False)\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c251d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_v1 = pd.DataFrame(evaluation_rows)\n",
    "print(\"Rows collected:\", len(df_v1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "df_v1.to_csv(\"results/prompt_v1_results.csv\", index=False)\n",
    "\n",
    "print(\"Saved partial results successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Prompt V2\n",
    "with open(\"prompts/prompt_v2.txt\", \"r\") as f:\n",
    "    prompt_v2_template = f.read()\n",
    "\n",
    "# Reset evaluation list for V2\n",
    "evaluation_v2 = []\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    review_text = row['review']\n",
    "    actual_stars = row['actual_stars']\n",
    "    \n",
    "    prompt_filled = prompt_v2_template.replace(\"{review_text}\", review_text)\n",
    "    \n",
    "    llm_output = call_llm_openrouter(prompt_filled)\n",
    "    parsed_json, valid_flag = parse_json_safe(llm_output) if llm_output else (None, False)\n",
    "    \n",
    "    evaluation_v2.append({\n",
    "        \"prompt_version\": \"v2\",\n",
    "        \"review\": review_text,\n",
    "        \"actual_stars\": actual_stars,\n",
    "        \"predicted_stars\": parsed_json.get(\"predicted_stars\") if parsed_json else None,\n",
    "        \"explanation\": parsed_json.get(\"explanation\") if parsed_json else None,\n",
    "        \"json_valid\": valid_flag,\n",
    "        \"llm_raw_output\": llm_output\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_v2_df = pd.DataFrame(evaluation_v2)\n",
    "results_v2_df.to_csv(\"results/evaluation_v2.csv\", index=False)\n",
    "results_v2_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4261e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/prompt_v3.txt\", \"r\") as f:\n",
    "    prompt_v3_template = f.read()\n",
    "\n",
    "evaluation_v3 = []\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    review_text = row['review']\n",
    "    actual_stars = row['actual_stars']\n",
    "    \n",
    "    prompt_filled = prompt_v3_template.replace(\"{review_text}\", review_text)\n",
    "    \n",
    "    llm_output = call_llm_openrouter(prompt_filled)\n",
    "    parsed_json, valid_flag = parse_json_safe(llm_output) if llm_output else (None, False)\n",
    "    \n",
    "    evaluation_v3.append({\n",
    "        \"prompt_version\": \"v3\",\n",
    "        \"review\": review_text,\n",
    "        \"actual_stars\": actual_stars,\n",
    "        \"predicted_stars\": parsed_json.get(\"predicted_stars\") if parsed_json else None,\n",
    "        \"explanation\": parsed_json.get(\"explanation\") if parsed_json else None,\n",
    "        \"json_valid\": valid_flag,\n",
    "        \"llm_raw_output\": llm_output\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_v3_df = pd.DataFrame(evaluation_v3)\n",
    "results_v3_df.to_csv(\"results/evaluation_v3.csv\", index=False)\n",
    "results_v3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df):\n",
    "    if df.empty:\n",
    "        return {\n",
    "            \"accuracy\": 0,\n",
    "            \"json_validity\": 0\n",
    "        }\n",
    "\n",
    "    required_cols = {\"predicted_stars\", \"actual_stars\", \"json_valid\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        return {\n",
    "            \"accuracy\": 0,\n",
    "            \"json_validity\": 0\n",
    "        }\n",
    "\n",
    "    total = len(df)\n",
    "    accuracy = (df[\"predicted_stars\"] == df[\"actual_stars\"]).sum()\n",
    "    valid_json = df[\"json_valid\"].sum()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": round((accuracy / total) * 100, 2),\n",
    "        \"json_validity\": round((valid_json / total) * 100, 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all evaluations into one DataFrame\n",
    "final_results_df = pd.concat([pd.DataFrame(evaluation_rows),\n",
    "                              results_v2_df,\n",
    "                              results_v3_df], ignore_index=True)\n",
    "\n",
    "# Save final CSV\n",
    "final_results_df.to_csv(\"results/final_task1_results.csv\", index=False)\n",
    "print(\"Final Task 1 results saved: results/final_task1_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31940975",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison.to_csv(\"results/comparison_table.csv\", index=False)\n",
    "print(\"Comparison table saved: results/comparison_table.csv\")\n",
    "comparison\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
